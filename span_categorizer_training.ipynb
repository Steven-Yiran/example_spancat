{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning A Bert Model for Span Categorization\n",
    "\n",
    "This notebook aims to demonstrate how to finetune a Span Categorization Model with BERT and use it to produce out-of-sample predicted probabilities for each token of each span class. These are required to find label issues in span classification dataset with cleanlab. The specific span classification task we consider here is Extractive Question Answering with the SQuAD dataset, and we train a Transformer model from HuggingFace's transformers library. This notebook demonstrates how to produce the `pred_probs`, using them to find label issues is demonstrated in `\"find label errors in span classification dataset\"` tutorial.\n",
    "\n",
    "***Note: running this notebook requires the .py files in the same folder.***\n",
    "\n",
    "Overview of what we'll do in this notebook:\n",
    "    - Read and process an Extractive Question Answering dataset.\n",
    "    - Compute out-of-sample predicted probability by training a BERT transformer model via cross-validation\n",
    "    - Separate question and context tokens for more meaningful error detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiran.shi/dev/example_spancat/ENV/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "from cleanlab.internal.token_classification_utils import process_token\n",
    "\n",
    "from spanbert import QA, QATrainer\n",
    "from span_classification_tutorial_utils import to_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 100\n",
    "NUM_SPLITS = 3\n",
    "model_folder_path = \"./folds\"\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\", split=f\"train[:{NUM_EXAMPLES}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `context`, `question`, and `answers` fields in the dataset. Let's print the first example in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "Question:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Answers:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Context: \", raw_datasets[0][\"context\"])\n",
    "print(\"Question: \", raw_datasets[0][\"question\"])\n",
    "print(\"Answers: \", raw_datasets[0][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `context` and `question` fields are straightforward to use. The `answers` field is in format that is used by common span classification datasets. The `text` field is the actual answer to the question in the context and the `answer_start` field contains the starting character index of the answer in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSpanClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No label map found, please train the model with the QATrainer class\n"
     ]
    }
   ],
   "source": [
    "label2id = {'O': 0, 'ANS': 1}\n",
    "id2label = {v:k for k, v in label2id.items()}\n",
    "max_length = 384\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "model = QA(model_checkpoint)\n",
    "trainer = QATrainer(model, max_length, label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be training a custom span classification that is based on the BERT model. Our custom span classification model will help us produce predicted probabilities needed for each token. You can check out more details about the model in the \"spanbert.py\" file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Span Classification Model using Cross-Validation\n",
    "\n",
    "To compute out-of-sample predicted probabilities (`pred_probs`) using cross validation, we first partition the dataset into `k = 3` disjoint folds and train one span classification model on each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 66/66 [00:00<00:00, 586.33 examples/s]\n",
      "100%|██████████| 27/27 [09:12<00:00, 20.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 552.6949, 'train_samples_per_second': 0.358, 'train_steps_per_second': 0.049, 'train_loss': 0.08430206334149395, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67/67 [00:00<00:00, 506.23 examples/s]\n",
      "100%|██████████| 27/27 [11:06<00:00, 24.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 666.4771, 'train_samples_per_second': 0.302, 'train_steps_per_second': 0.041, 'train_loss': 0.021081337222346553, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67/67 [00:00<00:00, 396.85 examples/s]\n",
      "100%|██████████| 27/27 [12:21<00:00, 27.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 741.4954, 'train_samples_per_second': 0.271, 'train_steps_per_second': 0.036, 'train_loss': 0.016514805731949984, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Define the number of splits for cross-validation\n",
    "kf = KFold(n_splits=NUM_SPLITS)\n",
    "splits = kf.split(raw_datasets)\n",
    "\n",
    "fold_to_ids = {} # store the validation ids for each fold\n",
    "for i, (train_ids, val_ids) in enumerate(splits):\n",
    "    fold_to_ids[i] = val_ids\n",
    "    output_dir = os.path.join(model_folder_path, f\"model_fold_{i}\")\n",
    "    if os.path.exists(output_dir):\n",
    "        print(f\"Model for fold {i} already exists, skipping training\")\n",
    "    else:\n",
    "        train_ds = raw_datasets.select(train_ids)\n",
    "        trainer.train(\n",
    "            train_ds,\n",
    "            output_dir,\n",
    "            num_train_epochs=3, # 3 epochs is enough for demonstration\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Out-of-Sample Predicted Probabilities\n",
    "\n",
    "We obtain the predicted class probabilities for each token using the model where this token was held out from the training set. From our custom QA model, we collect the predicted probabilities, tokenized version of question and context, and their the correct label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = {}\n",
    "sentence_probs = {}\n",
    "sentence_labels = {}\n",
    "\n",
    "for i in range(NUM_SPLITS):\n",
    "    model_path = os.path.join(model_folder_path, f\"model_fold_{i}\")\n",
    "    model = QA(model_path)\n",
    "\n",
    "    indices = fold_to_ids[i]\n",
    "    val_ds = raw_datasets.select(indices)\n",
    "    for i, index in enumerate(indices):\n",
    "        sentence_probs[index], tokens, sentence_labels[index] = model.predict(val_ds[i])\n",
    "\n",
    "        replace = [('#', ''), ('``', '\"'), (\"''\", '\"')]\n",
    "        sentence_tokens[index] = [process_token(t, replace) for t in tokens]\n",
    "\n",
    "sentence_tokens = [sentence_tokens[i] for i in range(NUM_EXAMPLES)]\n",
    "sentence_probs = [sentence_probs[i] for i in range(NUM_EXAMPLES)]\n",
    "sentence_labels = [sentence_labels[i] for i in range(NUM_EXAMPLES)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Isolate Question and Context\n",
    "\n",
    "During prediction we give the model both the question and the context. However, only the context portion have labels in the original SQuAD dataset. So, we separete the `pred_probs` for questions and context and only look at the context related `pred_probs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the list of tokens identify the index of the first [SEP] token\n",
    "sentence_questions = [q[\"question\"] for q in raw_datasets]\n",
    "\n",
    "for i in range(len(sentence_tokens)):\n",
    "    sep_idx = sentence_tokens[i].index('[SEP]')\n",
    "    sentence_tokens[i] = sentence_tokens[i][sep_idx + 1:-1]\n",
    "    sentence_probs[i] = sentence_probs[i][sep_idx + 1:-1]\n",
    "    sentence_labels[i] = sentence_labels[i][sep_idx + 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted the `pred_probs` for both the answer span (`ANS`) and the other span (`O`). We only care about the `ANS` span tokens so we isolate their `pred_probs` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_labels = []\n",
    "for labels in sentence_labels:\n",
    "    temp = [lab[label2id[\"ANS\"]] for lab in labels]\n",
    "    final_labels.append(temp)\n",
    "\n",
    "final_pred_probs = []\n",
    "for probs in sentence_probs:\n",
    "    temp = [prob[label2id[\"ANS\"]] for prob in probs]\n",
    "    final_pred_probs.append(np.array(temp))\n",
    "\n",
    "pred_probs_dict = to_dict(final_pred_probs)\n",
    "labels_dict = to_dict(final_labels)\n",
    "tokens_dict = to_dict(sentence_tokens)\n",
    "question_dict = to_dict(sentence_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we obtained properly formatted `pred_probs`, `labels`, `tokens`, and `questions` for use with `cleanlab.experimental.span_classification`. We use the `to_dict` function to convert them into a suitable format to save as `.npz` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('pred_probs.npz', **pred_probs_dict)\n",
    "np.savez('labels.npz', **labels_dict)\n",
    "np.savez('tokens.npz', **tokens_dict)\n",
    "np.savez('questions.npz', **question_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
